{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicodecoker/Transformers-Tutorials/blob/master/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1mpReuyRZHS"
      },
      "source": [
        "In this notebook, we are going to fine-tune `LayoutLMForSequenceClassification` on the [RVL-CDIP dataset](https://www.cs.cmu.edu/~aharley/rvl-cdip/), which is a document image classification task. Each scanned document in the dataset belongs to one of 16 classes, such as \"resume\" or \"invoice\" (so it's a multiclass classification problem). The entire dataset consists of no less than 400,000 (!) scanned documents.\n",
        "\n",
        "For demonstration purposes, we are going to fine-tune the model on a really small subset (one example per class), and verify whether the model is able to overfit them. Note that LayoutLM achieves state-of-the-art results on RVL-CDIP, with a classification accuracy of 94.42% on the test set.\n",
        "\n",
        "* Original LayoutLM paper: https://arxiv.org/abs/1912.13318\n",
        "* LayoutLM docs in the Transformers library: https://huggingface.co/transformers/model_doc/layoutlm.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb9O-5FdoCF9"
      },
      "source": [
        "## Setting up environment\n",
        "\n",
        "First, we install the ðŸ¤— transformers and datasets libraries, as well as the [Tesseract OCR engine](https://github.com/tesseract-ocr/tesseract) (built by Google). LayoutLM requires an external OCR engine of choice to turn a document into a list of words and bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9ALDa_1rrQO"
      },
      "source": [
        "! pip install transformers datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vbRjhABjF0J"
      },
      "source": [
        "## Getting the data\n",
        "\n",
        "Next, we download a small subset of the RVL-CDIP dataset (which I prepared), containing 15 documents (one example per class). I omitted the \"handwritten\" class, because the OCR results were mediocre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqaqmc3bQ2jr"
      },
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"RIPS-Goog-23/RVL-CDIP\", split=\"train[:1000]\")\n",
        "dataset.save_to_disk(\"datasets/rvl-cdip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rLqUaPU56s_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.save_to_disk(\"/content/drive/MyDrive/datasets/rvl-cdip\")"
      ],
      "metadata": {
        "id": "ZCz2MLZV52ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "dataset = load_from_disk(\"/content/drive/MyDrive/datasets/rvl-cdip\")"
      ],
      "metadata": {
        "id": "hCrwkoxG89Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN-iboovriQL"
      },
      "source": [
        "## Preprocessing the data using ðŸ¤— datasets\n",
        "\n",
        "First, we convert the dataset into a Pandas dataframe, having 2 columns: `image_path` and `label`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.features"
      ],
      "metadata": {
        "id": "J3u5BjZT0BaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey4xYRCCrXpo"
      },
      "source": [
        "labels = list(set(dataset['label']))\n",
        "idx2label = {v: k for v, k in enumerate(labels)}\n",
        "label2idx = {k: v for v, k in enumerate(labels)}\n",
        "label2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GbjY0kRSIFx"
      },
      "source": [
        "Next, we can turn the word-level 'words' and 'bbox' columns into token-level `input_ids`, `attention_mask`, `bbox` and `token_type_ids` using `LayoutLMTokenizer`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]['bbox']"
      ],
      "metadata": {
        "id": "6_NbdRgSDHkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_bbox(batch):\n",
        "  bbox_new = []\n",
        "  for bbox in batch[\"bbox\"]:\n",
        "    bbox_new.append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
        "  return {\"bbox\": bbox_new}\n",
        "\n",
        "dataset = dataset.map(convert_bbox, batched=False)"
      ],
      "metadata": {
        "id": "PTzb4E-6CpeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]['bbox']"
      ],
      "metadata": {
        "id": "vmJ5O71UDBMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "dataset = dataset.map(lambda sample: {'words': literal_eval(sample['text'])})\n",
        "dataset.features"
      ],
      "metadata": {
        "id": "GRohohHXyLDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg9vgcnBKC6Y"
      },
      "source": [
        "from transformers import LayoutLMTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = LayoutLMTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
        "\n",
        "def encode_example(example, max_seq_length=512, pad_token_box=[0, 0, 0, 0]):\n",
        "  words = example['words']\n",
        "  normalized_word_boxes = example['bbox']\n",
        "\n",
        "  assert len(words) == len(normalized_word_boxes)\n",
        "\n",
        "  token_boxes = []\n",
        "  for word, box in zip(words, normalized_word_boxes):\n",
        "      word_tokens = tokenizer.tokenize(word)\n",
        "      token_boxes.extend([box] * len(word_tokens))\n",
        "\n",
        "  # Truncation of token_boxes\n",
        "  special_tokens_count = 2\n",
        "  if len(token_boxes) > max_seq_length - special_tokens_count:\n",
        "      token_boxes = token_boxes[: (max_seq_length - special_tokens_count)]\n",
        "\n",
        "  # add bounding boxes of cls + sep tokens\n",
        "  token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n",
        "\n",
        "  encoding = tokenizer(' '.join(words), padding='max_length', truncation=True)\n",
        "  # Padding of token_boxes up the bounding boxes to the sequence length.\n",
        "  input_ids = tokenizer(' '.join(words), truncation=True)[\"input_ids\"]\n",
        "  padding_length = max_seq_length - len(input_ids)\n",
        "  token_boxes += [pad_token_box] * padding_length\n",
        "  encoding['bbox'] = token_boxes\n",
        "  encoding['label'] = label2idx[example['label']]\n",
        "\n",
        "  assert len(encoding['input_ids']) == max_seq_length\n",
        "  assert len(encoding['attention_mask']) == max_seq_length\n",
        "  assert len(encoding['token_type_ids']) == max_seq_length\n",
        "  assert len(encoding['bbox']) == max_seq_length\n",
        "\n",
        "  return encoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeksmkWwfKjH"
      },
      "source": [
        "from datasets import Features, Sequence, ClassLabel, Value, Array2D\n",
        "\n",
        "# we need to define the features ourselves as the bbox of LayoutLM are an extra feature\n",
        "features = Features({\n",
        "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
        "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
        "    'attention_mask': Sequence(Value(dtype='int64')),\n",
        "    'token_type_ids': Sequence(Value(dtype='int64')),\n",
        "    'label': ClassLabel(names=labels),\n",
        "    'image': Value(dtype='string'),\n",
        "    'text': Value(dtype='string'),\n",
        "    'words': Sequence(feature=Value(dtype='string')),\n",
        "})\n",
        "\n",
        "encoded_dataset = dataset.map(lambda example: encode_example(example),\n",
        "                              features=features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA8ZiWcr2Rwo"
      },
      "source": [
        "Finally, we set the format to PyTorch, as the LayoutLM implementation in the Transformers library is in PyTorch. We also specify which columns we are going to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWg6ArO003By"
      },
      "source": [
        "encoded_dataset.set_format(type='torch', columns=['input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1Qdjf8o37IN"
      },
      "source": [
        "dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=1, shuffle=True)\n",
        "batch = next(iter(dataloader))\n",
        "batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxmfktUy-o_E"
      },
      "source": [
        "Let's verify whether the input ids are created correctly by decoding them back to text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rJJ9WU7yTZC"
      },
      "source": [
        "tokenizer.decode(batch['input_ids'][0].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qGimR6HyZMy"
      },
      "source": [
        "idx2label[batch['label'][0].item()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28KGi6lqid0c"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "Here we define the model, namely `LayoutLMForSequenceClassification`. We initialize it with the weights of the pre-trained base model (`LayoutLMModel`). The weights of the classification head are randomly initialized, and will be fine-tuned together with the weights of the base model on our tiny dataset. Once loaded, we move it to the GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlb_izEJjy3Z"
      },
      "source": [
        "from transformers import LayoutLMForSequenceClassification\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = \"cpu\"\n",
        "print(f'Running on {device}')\n",
        "\n",
        "model = LayoutLMForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/layoutlm-base-uncased\",\n",
        "    num_labels=len(label2idx))\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE9wo9Y1j3i8"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Here we train the model in familiar PyTorch fashion. We use the Adam optimizer with weight decay fix (normally you can also specify which variables should have weight decay and which not + a learning rate scheduler, see [here](https://github.com/microsoft/unilm/blob/5d16c846bec56b6e88ec7de4fc3ceb7c803571a4/layoutlm/examples/classification/run_classification.py#L94) for how the authors of LayoutLM did this), and train for 30 epochs. If the model is able to overfit it, then it means there are no issues and we can train it on the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QZpQzFNj63T"
      },
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "global_step = 0\n",
        "num_train_epochs = 30\n",
        "t_total = len(dataloader) * num_train_epochs # total number of training steps\n",
        "\n",
        "#put the model in training mode\n",
        "model.train()\n",
        "for epoch in tqdm(range(num_train_epochs), position=0, leave=True):\n",
        "  print(\"Epoch:\", epoch)\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  for batch in tqdm(dataloader, position=0, leave=True):\n",
        "      input_ids = batch[\"input_ids\"].to(device)\n",
        "      bbox = batch[\"bbox\"].to(device)\n",
        "      attention_mask = batch[\"attention_mask\"].to(device)\n",
        "      token_type_ids = batch[\"token_type_ids\"].to(device)\n",
        "      labels = batch[\"label\"].to(device)\n",
        "\n",
        "      # forward pass\n",
        "      outputs = model(input_ids=input_ids,\n",
        "                      bbox=bbox,\n",
        "                      attention_mask=attention_mask,\n",
        "                      token_type_ids=token_type_ids,\n",
        "                      labels=labels)\n",
        "      loss = outputs.loss\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      predictions = outputs.logits.argmax(-1)\n",
        "      correct += (predictions == labels).float().sum()\n",
        "\n",
        "      # backward pass to get the gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # update\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      global_step += 1\n",
        "\n",
        "  print(\"Loss:\", running_loss / batch[\"input_ids\"].shape[0])\n",
        "  accuracy = 100 * correct / len(data)\n",
        "  print(\"Training accuracy:\", accuracy.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(output_dir=\"layoutlm-sequenceclassification-rvlcdip\",\n",
        "                         overwrite_output_dir=True,\n",
        "                         remove_unused_columns=False,\n",
        "                         warmup_steps=0.1,\n",
        "                         max_steps=2000,\n",
        "                         evaluation_strategy=\"steps\",\n",
        "                         eval_steps=100,\n",
        "                         push_to_hub=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )"
      ],
      "metadata": {
        "id": "k7H3ipK-6Ny5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}